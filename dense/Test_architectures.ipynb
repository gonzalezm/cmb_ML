{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1563953100660,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "Q6HUlw8kOrGA",
    "outputId": "e4b52046-aa6d-4268-9a28-e0819fbdfae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85439,
     "status": "ok",
     "timestamp": 1563953183190,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "tn4aEC7S7ns6",
    "outputId": "559daef5-4c27-44bc-e178-ee5fc26c25dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/gonzalezm/NNhealpix.git\n",
      "  Cloning https://github.com/gonzalezm/NNhealpix.git to /tmp/pip-req-build-t5_l0gzc\n",
      "  Running command git clone -q https://github.com/gonzalezm/NNhealpix.git /tmp/pip-req-build-t5_l0gzc\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (0.40.1)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.0 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (1.16.4)\n",
      "Requirement already satisfied: keras>=2.2 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (2.2.4)\n",
      "Collecting healpy>=1.12 (from nnhealpix==0.1.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/ac/3ec0ce2a9755a934888af7b6296aed030f7d3acdd4812fc9f40a6151b67f/healpy-1.12.9-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: tensorflow>=1.10 in /usr/local/lib/python3.6/dist-packages (from nnhealpix==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.39->nnhealpix==0.1.0) (0.29.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->nnhealpix==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->nnhealpix==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->nnhealpix==0.1.0) (2.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0->nnhealpix==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2->nnhealpix==0.1.0) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.2->nnhealpix==0.1.0) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2->nnhealpix==0.1.0) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.2->nnhealpix==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.2->nnhealpix==0.1.0) (2.8.0)\n",
      "Requirement already satisfied: astropy in /usr/local/lib/python3.6/dist-packages (from healpy>=1.12->nnhealpix==0.1.0) (3.0.5)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (0.33.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (0.7.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (0.1.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (1.11.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10->nnhealpix==0.1.0) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0->nnhealpix==0.1.0) (41.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.10->nnhealpix==0.1.0) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.10->nnhealpix==0.1.0) (3.1.1)\n",
      "Building wheels for collected packages: nnhealpix\n",
      "  Building wheel for nnhealpix (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4k6tujtu/wheels/01/64/0c/e09ec3f7352cf0b6494c569f85c43fe93ed1e56b49db38a3d7\n",
      "Successfully built nnhealpix\n",
      "Installing collected packages: healpy, nnhealpix\n",
      "Successfully installed healpy-1.12.9 nnhealpix-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/gonzalezm/NNhealpix.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 129979,
     "status": "ok",
     "timestamp": 1563953227752,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "F67wI6OoSJ-6",
    "outputId": "03ca76c5-c9a3-4b13-9427-933e5860aec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camb\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/f1/5ee8519ca1d5bc0bdd0bf42b7a11d89b7e31e9f2b9639435d53c41942f1c/camb-1.0.6.tar.gz (32.0MB)\n",
      "\u001b[K     |████████████████████████████████| 32.0MB 41.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from camb) (1.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from camb) (1.12.0)\n",
      "Requirement already satisfied: sympy>=1.0 in /usr/local/lib/python3.6/dist-packages (from camb) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=1.0->camb) (1.16.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy>=1.0->camb) (1.1.0)\n",
      "Building wheels for collected packages: camb\n",
      "  Building wheel for camb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/a7/8f/c92df2b95a9978e184f898c1e971dc100c9d8856fb01358d8b\n",
      "Successfully built camb\n",
      "Installing collected packages: camb\n",
      "Successfully installed camb-1.0.6\n",
      "Requirement already satisfied: healpy in /usr/local/lib/python3.6/dist-packages (1.12.9)\n",
      "Requirement already satisfied: astropy in /usr/local/lib/python3.6/dist-packages (from healpy) (3.0.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from healpy) (1.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from healpy) (1.12.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from healpy) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from healpy) (1.16.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->healpy) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->healpy) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->healpy) (2.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->healpy) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->healpy) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install camb\n",
    "!pip install healpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 138509,
     "status": "ok",
     "timestamp": 1563953236303,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "93_57tJmR8gd",
    "outputId": "414c9d10-d7a3-4be3-9d87-5e7014e137a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n",
      "unlensed_scalar\n",
      "unlensed_total\n",
      "lensed_scalar\n",
      "tensor\n",
      "lens_potential\n",
      "(2551, 4)\n"
     ]
    }
   ],
   "source": [
    "import camb\n",
    "from camb import model, initialpower\n",
    "import healpy as hp \n",
    "import numpy as np\n",
    "from pylab import *\n",
    "rcParams['image.cmap'] = 'jet'\n",
    "\n",
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=67.5, ombh2=0.022, omch2=0.122, mnu=0.06, omk=0, tau=0.06)\n",
    "pars.InitPower.set_params(ns=0.965, r=0)\n",
    "pars.set_for_lmax(2500, lens_potential_accuracy=0)\n",
    "#calculate results for these parameters\n",
    "results = camb.get_results(pars)\n",
    "#get dictionary of CAMB power spectra\n",
    "powers =results.get_cmb_power_spectra(pars, CMB_unit='muK')\n",
    "for name in powers: print(name)\n",
    "\n",
    "\n",
    "#plot the total lensed CMB power spectra versus unlensed, and fractional difference\n",
    "totCL=powers['total']\n",
    "unlensedCL=powers['unlensed_scalar']\n",
    "print(totCL.shape)\n",
    "\n",
    "ls = np.arange(totCL.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 138496,
     "status": "ok",
     "timestamp": 1563953236313,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "fx0qn2TqSPX8",
    "outputId": "1434e582-4936-4594-bdf9-d35fc4883586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(ls[0])\n",
    "CL = totCL[:,0]/ls/(ls+1)\n",
    "CL[0]=0\n",
    "\n",
    "ns = 16\n",
    "lmax = 2*ns-1\n",
    "nl = 2*ns\n",
    "nalm = (nl)*(nl+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kW0go6ajSTv0"
   },
   "outputs": [],
   "source": [
    "### Target power spectra\n",
    "\n",
    "nbmodels = 55000\n",
    "nnn = int(nbmodels/30)\n",
    "npixok = 12*ns**2\n",
    "limit_shape = 3*ns\n",
    "okpix = np.arange(npixok)\n",
    "mymaps = np.zeros((nbmodels, npixok))\n",
    "myalms = np.zeros((nbmodels, int(nalm)), dtype=complex128)\n",
    "expcls = np.zeros((nbmodels, nl))\n",
    "mycls = np.zeros((nbmodels, nl))\n",
    "allshapes = np.zeros((nbmodels, len(ls)))\n",
    "for i in range(nbmodels):\n",
    "  ylo = np.random.rand()*2\n",
    "  yhi = np.random.rand()*2\n",
    "  theshape = ylo+(yhi-ylo)/(limit_shape)*ls\n",
    "  theshape[theshape < 0] = 0\n",
    "  theshape[limit_shape:] = 0\n",
    "  allshapes[i,:] = theshape\n",
    "  theCL = CL*theshape\n",
    "  themap = hp.synfast(theCL, ns, pixwin=False, verbose=False)\n",
    "  mymaps[i,:] = themap[okpix]\n",
    "  expcls[i,:], myalms[i,:] = hp.anafast(themap, lmax=lmax, alm=True)\n",
    "  mycls[i,:] = theCL[0:nl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4WmxTKSSWKo"
   },
   "outputs": [],
   "source": [
    "num_out = mycls.shape[1]\n",
    "shape=(len(mymaps[0,:]),1)\n",
    "\n",
    "### Deep learning on CNN architecture\n",
    "import keras as kr\n",
    "import nnhealpix.layers\n",
    "\n",
    "\n",
    "#NBB layers\n",
    "inputs=kr.layers.Input(shape)\n",
    "x=inputs\n",
    "for i in range (int(math.log(ns,2))):\n",
    "#Recog of the neighbours & Convolution\n",
    "    print(int(ns/(2**(i))), int(ns/(2**(i+1))))\n",
    "    x = nnhealpix.layers.ConvNeighbours(int(ns/(2**(i))), filters=32, kernel_size=9)(x)\n",
    "    x = kr.layers.Activation('relu')(x)\n",
    "#Degrade\n",
    "    x = nnhealpix.layers.AveragePooling(int(ns/(2**(i))), int(ns/(2**(i+1))))(x)\n",
    "#End of the NBBs\n",
    "x = kr.layers.Dropout(0.2)(x)\n",
    "x = kr.layers.Flatten()(x)\n",
    "x = kr.layers.Dense(1024)(x)\n",
    "x = kr.layers.Activation('relu')(x)\n",
    "x = kr.layers.Dense(num_out)(x)\n",
    "\n",
    "out=kr.layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "nbtest = 5000\n",
    "\n",
    "mapx = (mymaps-np.mean(mymaps))/np.std(mymaps)\n",
    "clstd = np.std(mycls)\n",
    "clmean = np.mean(mycls)\n",
    "cly = (mycls-np.mean(mycls))/np.std(mycls)\n",
    "\n",
    "mapx = mapx.reshape(mapx.shape[0], len(mapx[0]), 1)\n",
    "\n",
    "mymaps = mymaps.reshape(mymaps.shape[0], len(mymaps[0]), 1)\n",
    "\n",
    "class PrintNum(kr.callbacks.Callback):\n",
    "  def on_epoch_end(self,epoch,logs):\n",
    "    if (epoch) % 10 == 0: \n",
    "      print(epoch+1)\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "stop = kr.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  verbose = 0,\n",
    "                                  restore_best_weights=True,\n",
    "                                  patience=20)\n",
    "\n",
    "callbacks = [PrintNum(), stop]\n",
    "\n",
    "\n",
    "# Creation of the model\n",
    "model = kr.models.Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss=kr.losses.mse, optimizer='adam', metrics=[kr.metrics.mean_absolute_percentage_error])\n",
    "\n",
    "# Training\n",
    "history = model.fit(mapx[:(nbmodels-nbtest),:,:], mycls[:(nbmodels-nbtest),:], epochs=20, batch_size=32, validation_split = 0.1, verbose = 0, callbacks=callbacks, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKY5ubd5_pQV"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plot(history.history['loss'])\n",
    "plot(history.history['val_loss'])\n",
    "title('model loss')\n",
    "ylabel('loss')\n",
    "xlabel('epoch')\n",
    "legend(['train', 'test'], loc='upper left')\n",
    "yscale('log')\n",
    "show()\n",
    "print(min(history.history['loss']), min(history.history['val_loss']), len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VNLYqcLISpee"
   },
   "outputs": [],
   "source": [
    "mymaps_test = mapx[(nbmodels-nbtest):,:,:]\n",
    "mycls_test = mycls[(nbmodels-nbtest):,:]\n",
    "expcls_test = expcls[(nbmodels-nbtest):,:]\n",
    "\n",
    "result = model.predict(mymaps_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-ORo2j9yF9P"
   },
   "outputs": [],
   "source": [
    "print(result.shape)\n",
    "print(clmean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcL2dYRdStZI"
   },
   "outputs": [],
   "source": [
    "mean_err = np.mean(abs((result[:,2:]-mycls_test[:,2:])/mycls_test[:,2:]*100))\n",
    "mean_err_ana = np.mean(abs((expcls_test[:,2:]-mycls_test[:,2:])/mycls_test[:,2:]*100))\n",
    "print(mean_err)\n",
    "print(mean_err_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WV6t5Lt7Swxp"
   },
   "outputs": [],
   "source": [
    "lt=np.arange(nl)\n",
    "num=np.random.randint(result.shape[0])\n",
    "plot(lt, lt*(lt+1)*mycls_test[num,:],label ='Input spectra')\n",
    "plot(lt, lt*(lt+1)*expcls_test[num,:],label ='Anafast')\n",
    "plot(lt, lt*(lt+1)*result[num,:],label ='ML')\n",
    "title(num)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmNKKnV-S119"
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "a=hist(np.ravel(expcls_test[:,2:]-mycls_test[:,2:]), bins=100, alpha=0.5, label='Anafast')\n",
    "a=hist(np.ravel(result[:,2:]-mycls_test[:,2:]), bins=100, alpha=0.5, label = 'ML')\n",
    "yscale('log')\n",
    "legend()\n",
    "\n",
    "ch2anafast = np.sum((expcls_test[:,2:]-mycls_test[:,2:])**2, axis=1)\n",
    "ch2ML = np.sum((result[:,2:]-mycls_test[:,2:])**2, axis=1)\n",
    "\n",
    "figure()\n",
    "a=hist(ch2anafast, bins=100, alpha=0.5, label='Anafast')\n",
    "a=hist(ch2ML, bins=100, alpha=0.5, label = 'ML')\n",
    "yscale('log')\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtweGStHS2f0"
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "import keras as kr\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "out_dir=\"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "\n",
    "kr.models.save_model(model, out_dir +  \"test20\" + \"_mymodel_CNN.h5py.File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Gkt19vzS8bT"
   },
   "outputs": [],
   "source": [
    "np.save(out_dir + \"test20\" + '_hist_loss', history.history['loss'])\n",
    "np.save(out_dir + \"test20\" + '_hist_val_loss', history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzyKE9PjS9EL"
   },
   "source": [
    "# Tests sur réseaux denses avec mymaps en input\n",
    "\n",
    "``` python\n",
    "model = Sequential()\n",
    "\n",
    "from keras.layers import Dense\n",
    "model.add(Dense(units=nalm*6,activation='relu',input_dim=npixok,kernel_initializer='uniform'))\n",
    "model.add(Dense(units=nalm*4, activation='relu'))\n",
    "model.add(Dense(units=nl, activation='linear'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "# Test 1:\n",
    "Deux couches dense de nalm*6 et nl neurones avec nl = 2*ns et nalm=(nl)*(nl+1)/2.\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un spli de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats1: \n",
    "Erreur absolue moyenne relative de :\n",
    "### 20,53%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,30%\n",
    "\n",
    "# Test 2:\n",
    "Deux couches dense de nalm10 et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats2:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 45,77%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,30%\n",
    "\n",
    "# Test3:\n",
    "Deux couches dense de nalm3 et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats3:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 61,00%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,32%\n",
    "\n",
    "En changeant le nombre de neurones de la première couche on a une modification de la performance mais pas de différence notable sur le chi2 ou l'histogramme de l'erreur si ce n'est des valeur légèrement différente (en accord ave la nouvelle erreur). Il semble que 6*nalme soit la meilleur quantité de neurones.\n",
    "\n",
    "\n",
    "\n",
    "# Test4:\n",
    "Trois couches dense de nalm*6, nalm*6 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats4:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 97.36667359705528%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.323505638289156%\n",
    "\n",
    "# Test5:\n",
    "Trois couches dense de nalm*6, nalm*8 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats5:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 70.17615607561682%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.323505638289156%\n",
    "\n",
    "# Test6:\n",
    "Trois couches dense de nalm*6, nalm*12 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats6:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 69.0112935047282%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.337909475916774%\n",
    "\n",
    "On a ajouté une couche de neurone dense. En faisant varier la quantié de neurones on observe\n",
    "\n",
    "# Test7:\n",
    "Trois couches dense de nalm*6, nalm*6 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats7:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 14.372369414690606%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.321932372640372%\n",
    "\n",
    "# Test8:\n",
    "Trois couches dense de nalm*6, nalm*8 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats8:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 15.105077069416465%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.276099533297497%\n",
    "\n",
    "# Test9:\n",
    "Trois couches dense de nalm*6, nalm*4 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats9:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 13.116186779395962%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.294983163667492%\n",
    "\n",
    "# Test10:\n",
    "Trois couches dense de nalm*6, nalm*2 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats10:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 18.824813082983688%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "# Test11:\n",
    "Trois couches dense de nalm*6, nalm*3 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats11:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 13.89%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8_rhxq-TCrI"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Au vu des tests réalisés, on observe qu'en conservant le nombre de neurones originaux pour la premiere couche, le programme est le plus performant (pour cette configuration). On remarque également qu'en ajoutant une couche dense de 4nalm neurones avec une fonction d'activation 'relu', les performance s'acroissent fortement (7% de gain). On pourrait ajouter de nouvelles couches denses 'relu' et faire varier leurs nombres de neurones pour améliorer les performances et diminuer l'erreur, cependant le temps necessaire pour réaliser les opération de la part de GGL Colab étant déjà long pour une couche supplémentaire on peut se douter qu'icelui augmentera encore. Au final si nous mettons plus de couches nous pourrions avoir un résultat plus smooth comme obtenu avec le CNN mais pour un temps beacoup plus long.\n",
    "\n",
    "Les différences notables entre les résultats de cette architecture et celle des réseau convolutionels se trouve sur le fit des Cl (normal). Avec le réseau dense on retrouve grossièrement le spectre recherché mais avec de légères oscillations semblable à du bruit. Avec le CNN on retrouve le spectre attendu mais avec des pics spontanés à certains points du spectre. Le manque de test sur le CNN pour ce problème ne me permet pas de le comparer efficacement avec le réseau dense, mais offre une idée des différences potentielle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLsSkRaxTKWL"
   },
   "source": [
    "\n",
    "# Tests avec réseau CNN NNhealpix, my maps en inputs\n",
    "\n",
    "```python\n",
    "\n",
    "#NBB layers\n",
    "inputs=kr.layers.Input(shape)\n",
    "x=inputs\n",
    "for i in range (int(math.log(ns,2))):\n",
    "#Recog of the neighbours & Convolution\n",
    "    print(int(ns/(2**(i))), int(ns/(2**(i+1))))\n",
    "    x = nnhealpix.layers.ConvNeighbours(int(ns/(2**(i))), filters=32, kernel_size=9)(x)\n",
    "    x = kr.layers.Activation('relu')(x)\n",
    "#Degrade\n",
    "    x = nnhealpix.layers.AveragePooling(int(ns/(2**(i))), int(ns/(2**(i+1))))(x)\n",
    "#End of the NBBs\n",
    "x = kr.layers.Dropout(0.2)(x)\n",
    "x = kr.layers.Flatten()(x)\n",
    "x = kr.layers.Dense(256)(x)\n",
    "x = kr.layers.Activation('relu')(x)\n",
    "x = kr.layers.Dense(num_out)(x)\n",
    "\n",
    "out=kr.layers.Activation('relu')(x)\n",
    "```\n",
    "\n",
    "\n",
    "# Test12:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data. Degrade jusqu'à 1.\n",
    "Test sur 10 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats12:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 55.12389643654892%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "# Test13:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data. Degrade jusqu'à 1.\n",
    "Test sur 20 epochs et batch_size=32. (MaxPooling et 48neurons)\n",
    "\n",
    "\n",
    "## Résultats13:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 61.199847644870964%. (Etrange car durant l'entrainement il a atteint 56%)\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "# Test14:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1.\n",
    "Test sur 10 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats14:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 28.295155920708048%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "On peut observer des pic vers 0 a certain androis et une forme smooth pour le reste. Pas d'oscillation chaotique autour de la courbe recherchée.\n",
    "\n",
    "# Test15:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1.\n",
    "Test sur 20 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats15:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 24.956576492551307%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "# Test16:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1.\n",
    "Test sur 30 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats16:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 24.472246345357334%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "Les pic vers 0 sont toujours presents mais il y en a moins 4 au lieu de 5. Celui en l=13 n'y est plus visiblement.\n",
    "\n",
    "# Test17:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1.\n",
    "Test sur 40 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats17:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 20.817694873906856%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "Toujours 4 pics à zero. Aux mêmes emplacements que durant le test 16. Meilleur qu'anafast moins bon que dense NN.\n",
    "\n",
    "# Test18:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1.\n",
    "Test sur 50 epochs et batch_size=32\n",
    "\n",
    "\n",
    "## Résultats18:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 23.889915106695522%.\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUVLbyZXJf44"
   },
   "source": [
    "# Tests avec réseau CNN NNhealpix, my maps en inputs\n",
    "# Tests sur le réseau\n",
    "\n",
    "55000 data\n",
    "\n",
    "# Test19:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 256 neurones dans la couche dense. Degrade jusqu'à 1. Test sur 20 epochs et batch_size=32\n",
    "\n",
    "## Résultats19:\n",
    "Mean absolute percentage error:\n",
    "\n",
    "### 17.67862693860468%\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "\n",
    "### ~22.294983163667492%\n",
    "\n",
    "Étrangement seulement 3 pic vers 0.\n",
    "\n",
    "# Test20:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data, AveragePooling et 512 neurones dans la couche dense. Degrade jusqu'à 1. Test sur 20 epochs et batch_size=32\n",
    "\n",
    "## Résultats20:\n",
    "Mean absolute percentage error:\n",
    "\n",
    "### %\n",
    "\n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "\n",
    "### ~22.294983163667492%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8QiLpQ6_YDJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Test_architectures.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
