{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5147,
     "status": "ok",
     "timestamp": 1562852657184,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "8FvWTuITh0RN",
    "outputId": "041d1fd5-da87-4ef0-bf25-a8195ac14cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'NNhealpix'...\n",
      "remote: Enumerating objects: 129, done.\u001b[K\n",
      "remote: Counting objects:   0% (1/129)   \u001b[K\r",
      "remote: Counting objects:   1% (2/129)   \u001b[K\r",
      "remote: Counting objects:   2% (3/129)   \u001b[K\r",
      "remote: Counting objects:   3% (4/129)   \u001b[K\r",
      "remote: Counting objects:   4% (6/129)   \u001b[K\r",
      "remote: Counting objects:   5% (7/129)   \u001b[K\r",
      "remote: Counting objects:   6% (8/129)   \u001b[K\r",
      "remote: Counting objects:   7% (10/129)   \u001b[K\r",
      "remote: Counting objects:   8% (11/129)   \u001b[K\r",
      "remote: Counting objects:   9% (12/129)   \u001b[K\r",
      "remote: Counting objects:  10% (13/129)   \u001b[K\r",
      "remote: Counting objects:  11% (15/129)   \u001b[K\r",
      "remote: Counting objects:  12% (16/129)   \u001b[K\r",
      "remote: Counting objects:  13% (17/129)   \u001b[K\r",
      "remote: Counting objects:  14% (19/129)   \u001b[K\r",
      "remote: Counting objects:  15% (20/129)   \u001b[K\r",
      "remote: Counting objects:  16% (21/129)   \u001b[K\r",
      "remote: Counting objects:  17% (22/129)   \u001b[K\r",
      "remote: Counting objects:  18% (24/129)   \u001b[K\r",
      "remote: Counting objects:  19% (25/129)   \u001b[K\r",
      "remote: Counting objects:  20% (26/129)   \u001b[K\r",
      "remote: Counting objects:  21% (28/129)   \u001b[K\r",
      "remote: Counting objects:  22% (29/129)   \u001b[K\r",
      "remote: Counting objects:  23% (30/129)   \u001b[K\r",
      "remote: Counting objects:  24% (31/129)   \u001b[K\r",
      "remote: Counting objects:  25% (33/129)   \u001b[K\r",
      "remote: Counting objects:  26% (34/129)   \u001b[K\r",
      "remote: Counting objects:  27% (35/129)   \u001b[K\r",
      "remote: Counting objects:  28% (37/129)   \u001b[K\r",
      "remote: Counting objects:  29% (38/129)   \u001b[K\r",
      "remote: Counting objects:  30% (39/129)   \u001b[K\r",
      "remote: Counting objects:  31% (40/129)   \u001b[K\r",
      "remote: Counting objects:  32% (42/129)   \u001b[K\r",
      "remote: Counting objects:  33% (43/129)   \u001b[K\r",
      "remote: Counting objects:  34% (44/129)   \u001b[K\r",
      "remote: Counting objects:  35% (46/129)   \u001b[K\r",
      "remote: Counting objects:  36% (47/129)   \u001b[K\r",
      "remote: Counting objects:  37% (48/129)   \u001b[K\r",
      "remote: Counting objects:  38% (50/129)   \u001b[K\r",
      "remote: Counting objects:  39% (51/129)   \u001b[K\r",
      "remote: Counting objects:  40% (52/129)   \u001b[K\r",
      "remote: Counting objects:  41% (53/129)   \u001b[K\r",
      "remote: Counting objects:  42% (55/129)   \u001b[K\r",
      "remote: Counting objects:  43% (56/129)   \u001b[K\r",
      "remote: Counting objects:  44% (57/129)   \u001b[K\r",
      "remote: Counting objects:  45% (59/129)   \u001b[K\r",
      "remote: Counting objects:  46% (60/129)   \u001b[K\r",
      "remote: Counting objects:  47% (61/129)   \u001b[K\r",
      "remote: Counting objects:  48% (62/129)   \u001b[K\r",
      "remote: Counting objects:  49% (64/129)   \u001b[K\r",
      "remote: Counting objects:  50% (65/129)   \u001b[K\r",
      "remote: Counting objects:  51% (66/129)   \u001b[K\r",
      "remote: Counting objects:  52% (68/129)   \u001b[K\r",
      "remote: Counting objects:  53% (69/129)   \u001b[K\r",
      "remote: Counting objects:  54% (70/129)   \u001b[K\r",
      "remote: Counting objects:  55% (71/129)   \u001b[K\r",
      "remote: Counting objects:  56% (73/129)   \u001b[K\r",
      "remote: Counting objects:  57% (74/129)   \u001b[K\r",
      "remote: Counting objects:  58% (75/129)   \u001b[K\r",
      "remote: Counting objects:  59% (77/129)   \u001b[K\r",
      "remote: Counting objects:  60% (78/129)   \u001b[K\r",
      "remote: Counting objects:  61% (79/129)   \u001b[K\r",
      "remote: Counting objects:  62% (80/129)   \u001b[K\r",
      "remote: Counting objects:  63% (82/129)   \u001b[K\r",
      "remote: Counting objects:  64% (83/129)   \u001b[K\r",
      "remote: Counting objects:  65% (84/129)   \u001b[K\r",
      "remote: Counting objects:  66% (86/129)   \u001b[K\r",
      "remote: Counting objects:  67% (87/129)   \u001b[K\r",
      "remote: Counting objects:  68% (88/129)   \u001b[K\r",
      "remote: Counting objects:  69% (90/129)   \u001b[K\r",
      "remote: Counting objects:  70% (91/129)   \u001b[K\r",
      "remote: Counting objects:  71% (92/129)   \u001b[K\r",
      "remote: Counting objects:  72% (93/129)   \u001b[K\r",
      "remote: Counting objects:  73% (95/129)   \u001b[K\r",
      "remote: Counting objects:  74% (96/129)   \u001b[K\r",
      "remote: Counting objects:  75% (97/129)   \u001b[K\r",
      "remote: Counting objects:  76% (99/129)   \u001b[K\r",
      "remote: Counting objects:  77% (100/129)   \u001b[K\r",
      "remote: Counting objects:  78% (101/129)   \u001b[K\r",
      "remote: Counting objects:  79% (102/129)   \u001b[K\r",
      "remote: Counting objects:  80% (104/129)   \u001b[K\r",
      "remote: Counting objects:  81% (105/129)   \u001b[K\r",
      "remote: Counting objects:  82% (106/129)   \u001b[K\r",
      "remote: Counting objects:  83% (108/129)   \u001b[K\r",
      "remote: Counting objects:  84% (109/129)   \u001b[K\r",
      "remote: Counting objects:  85% (110/129)   \u001b[K\r",
      "remote: Counting objects:  86% (111/129)   \u001b[K\r",
      "remote: Counting objects:  87% (113/129)   \u001b[K\r",
      "remote: Counting objects:  88% (114/129)   \u001b[K\r",
      "remote: Counting objects:  89% (115/129)   \u001b[K\r",
      "remote: Counting objects:  90% (117/129)   \u001b[K\r",
      "remote: Counting objects:  91% (118/129)   \u001b[K\r",
      "remote: Counting objects:  92% (119/129)   \u001b[K\r",
      "remote: Counting objects:  93% (120/129)   \u001b[K\r",
      "remote: Counting objects:  94% (122/129)   \u001b[K\r",
      "remote: Counting objects:  95% (123/129)   \u001b[K\r",
      "remote: Counting objects:  96% (124/129)   \u001b[K\r",
      "remote: Counting objects:  97% (126/129)   \u001b[K\r",
      "remote: Counting objects:  98% (127/129)   \u001b[K\r",
      "remote: Counting objects:  99% (128/129)   \u001b[K\r",
      "remote: Counting objects: 100% (129/129)   \u001b[K\r",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 863 (delta 90), reused 80 (delta 41), pack-reused 734\u001b[K\n",
      "Receiving objects: 100% (863/863), 22.51 MiB | 18.29 MiB/s, done.\n",
      "Resolving deltas: 100% (520/520), done.\n"
     ]
    }
   ],
   "source": [
    "#!pip install camb\n",
    "#!pip install healpy\n",
    "!git clone https://github.com/gonzalezm/NNhealpix.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2607,
     "status": "ok",
     "timestamp": 1562852508182,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "ww6ZiT_rQ9-i",
    "outputId": "e2cfdb13-dc40-49b5-8c44-d87713f9f484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "running egg_info\n",
      "creating nnhealpix.egg-info\n",
      "writing nnhealpix.egg-info/PKG-INFO\n",
      "writing top-level names to nnhealpix.egg-info/top_level.txt\n",
      "writing dependency_links to nnhealpix.egg-info/dependency_links.txt\n",
      "writing manifest file 'nnhealpix.egg-info/SOURCES.txt'\n",
      "error: package directory 'nnhealpix' does not exist\n"
     ]
    }
   ],
   "source": [
    " !python NNhealpix/setup.py develop --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4emXZv4rNHxR"
   },
   "outputs": [],
   "source": [
    "!rm -r nnhealpix.egg-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yw0Ib7KOPxro"
   },
   "outputs": [],
   "source": [
    "!cd NNhealpix/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2160,
     "status": "ok",
     "timestamp": 1562852634203,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "2GQLR6AvOMm_",
    "outputId": "b0035dc9-4bed-4855-b288-011a98bbce71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  .config\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 404,
     "status": "error",
     "timestamp": 1562852738602,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "YNvBh4RQNMCy",
    "outputId": "eed897fe-904d-4f4c-e17b-76f4688168f3"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1ccd19e514c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mNNhealpix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named NNhealpix",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import NNhealpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18314,
     "status": "ok",
     "timestamp": 1562850594207,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "IoFancfhh9HJ",
    "outputId": "c2705bc9-486c-44e4-c821-eea435ea5c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n",
      "lens_potential\n",
      "lensed_scalar\n",
      "unlensed_scalar\n",
      "unlensed_total\n",
      "tensor\n",
      "(2551, 4)\n"
     ]
    }
   ],
   "source": [
    "import camb\n",
    "from camb import model, initialpower\n",
    "import healpy as hp \n",
    "import numpy as np\n",
    "from pylab import *\n",
    "rcParams['image.cmap'] = 'jet'\n",
    "\n",
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=67.5, ombh2=0.022, omch2=0.122, mnu=0.06, omk=0, tau=0.06)\n",
    "pars.InitPower.set_params(ns=0.965, r=0)\n",
    "pars.set_for_lmax(2500, lens_potential_accuracy=0);\n",
    "#calculate results for these parameters\n",
    "results = camb.get_results(pars)\n",
    "#get dictionary of CAMB power spectra\n",
    "powers =results.get_cmb_power_spectra(pars, CMB_unit='muK')\n",
    "for name in powers: print(name)\n",
    "\n",
    "\n",
    "#plot the total lensed CMB power spectra versus unlensed, and fractional difference\n",
    "totCL=powers['total']\n",
    "unlensedCL=powers['unlensed_scalar']\n",
    "print(totCL.shape)\n",
    "\n",
    "ls = np.arange(totCL.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18270,
     "status": "ok",
     "timestamp": 1562850594230,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "7TvrWXl6h_ZB",
    "outputId": "d2ff87a2-1933-4c72-a29f-c3c54c491ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(ls[0])\n",
    "CL = totCL[:,0]/ls/(ls+1)\n",
    "CL[0]=0\n",
    "\n",
    "ns = 16\n",
    "lmax = 2*ns-1\n",
    "nl = 2*ns\n",
    "nalm = (nl)*(nl+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqYJB5whiEZ4"
   },
   "outputs": [],
   "source": [
    "### Target power spectra\n",
    "\n",
    "nbmodels = 110000\n",
    "nnn = int(nbmodels/30)\n",
    "npixok = 12*ns**2\n",
    "limit_shape = 3*ns\n",
    "okpix = np.arange(npixok)\n",
    "mymaps = np.zeros((nbmodels, npixok))\n",
    "myalms = np.zeros((nbmodels, nalm), dtype=complex128)\n",
    "expcls = np.zeros((nbmodels, nl))\n",
    "mycls = np.zeros((nbmodels, nl))\n",
    "allshapes = np.zeros((nbmodels, len(ls)))\n",
    "for i in range(nbmodels):\n",
    "  ylo = np.random.rand()*2\n",
    "  yhi = np.random.rand()*2\n",
    "  theshape = ylo+(yhi-ylo)/(limit_shape)*ls\n",
    "  theshape[theshape < 0] = 0\n",
    "  theshape[limit_shape:] = 0\n",
    "  allshapes[i,:] = theshape\n",
    "  theCL = CL*theshape\n",
    "  themap = hp.synfast(theCL, ns, pixwin=False, verbose=False)\n",
    "  mymaps[i,:] = themap[okpix]\n",
    "  expcls[i,:], myalms[i,:] = hp.anafast(themap, lmax=lmax, alm=True)\n",
    "  mycls[i,:] = theCL[0:nl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 473,
     "status": "error",
     "timestamp": 1562851326446,
     "user": {
      "displayName": "X.D.topher sama",
      "photoUrl": "https://lh5.googleusercontent.com/-bezrQ2Sgsnc/AAAAAAAAAAI/AAAAAAAACa4/mLbI1TQOnSM/s64/photo.jpg",
      "userId": "09059908567566041974"
     },
     "user_tz": -120
    },
    "id": "crgTOGoeiO3n",
    "outputId": "85009fe3-9979-49b5-f3be-6ec8e8f6ddf8"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1b51075001dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m### Deep learning on CNN architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mNNhealpix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#NBB layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named NNhealpix",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_out = mycls.shape[1]\n",
    "shape=(len(mymaps[0,:]),1)\n",
    "\n",
    "### Deep learning on CNN architecture\n",
    "import keras as kr\n",
    "import nnhealpix\n",
    "\n",
    "#NBB layers\n",
    "inputs=kr.layers.Input(shape)\n",
    "x=inputs\n",
    "for i in range (int(math.log(ns,2))):\n",
    "#Recog of the neighbours & Convolution\n",
    "    print(int(ns/(2**(i))), int(ns/(2**(i+1))))\n",
    "    x = nnhealpix.layers.ConvNeighbours(int(ns/(2**(i))), filters=32, kernel_size=9)(x)\n",
    "    x = kr.layers.Activation('relu')(x)\n",
    "#Degrade\n",
    "    x = nnhealpix.layers.MaxPooling(int(ns/(2**(i))), int(ns/(2**(i+1))))(x)\n",
    "#End of the NBBs\n",
    "x = kr.layers.Dropout(0.2)(x)\n",
    "x = kr.layers.Flatten()(x)\n",
    "x = kr.layers.Dense(48)(x)\n",
    "x = kr.layers.Activation('relu')(x)\n",
    "x = kr.layers.Dense(num_out)(x)\n",
    "\n",
    "out=kr.layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "nbtest = 1000\n",
    "\n",
    "mx = np.max(np.abs(mymaps))\n",
    "my = np.max(mycls)\n",
    "\n",
    "mymaps = mymaps.reshape(mymaps.shape[0], len(mymaps[0]), 1)\n",
    "\n",
    "class PrintNum(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self,epoch,logs):\n",
    "    if (epoch) % 10 == 0: \n",
    "      print(epoch)\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "stop = kr.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  verbose = 0,\n",
    "                                  restore_best_weights=True,\n",
    "                                  patience=20)\n",
    "\n",
    "callbacks = [PrintNum(), stop]\n",
    "\n",
    "\n",
    "# Creation of the model\n",
    "mymodel = kr.models.Model(inputs=inputs, outputs=out)\n",
    "mymodel.compile(loss=kr.losses.mse, optimizer='adam', metrics=[kr.metrics.mean_absolute_percentage_error])\n",
    "mymodel.summary()\n",
    "\n",
    "# Training\n",
    "history = model.fit(mymaps[:(nbmodels-nbtest),:,:], mycls[:(nbmodels-nbtest),:], epochs=100, batch_size=32, validation_split = 0.1, verbose = 1, callbacks=callbacks, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mY_SROHhnquB"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plot(history.history['loss'])\n",
    "plot(history.history['val_loss'])\n",
    "title('model loss')\n",
    "ylabel('loss')\n",
    "xlabel('epoch')\n",
    "legend(['train', 'test'], loc='upper left')\n",
    "yscale('log')\n",
    "show()\n",
    "print(min(history.history['loss']), min(history.history['val_loss']), len(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDECsUpViXJ8"
   },
   "outputs": [],
   "source": [
    "mymaps_test = mymaps[(nbmodels-nbtest):,:]\n",
    "mycls_test = mycls[(nbmodels-nbtest):,:]\n",
    "expcls_test = expcls[(nbmodels-nbtest):,:]\n",
    "\n",
    "result = my * model.predict(mymaps_test / mx, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Klsgle0HFeHc"
   },
   "outputs": [],
   "source": [
    "mean_err = np.mean(abs((result[:,2:]-mycls_test[:,2:])/mycls_test[:,2:]*100))\n",
    "mean_err_ana = np.mean(abs((expcls_test[:,2:]-mycls_test[:,2:])/mycls_test[:,2:]*100))\n",
    "print(mean_err)\n",
    "print(mean_err_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3uvqa_nBiXU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lt=np.arange(nl)\n",
    "num=np.random.randint(result.shape[0])\n",
    "plot(lt, lt*(lt+1)*mycls_test[num,:],label ='Input spectra')\n",
    "plot(lt, lt*(lt+1)*expcls_test[num,:],label ='Anafast')\n",
    "plot(lt, lt*(lt+1)*result[num,:],label ='ML')\n",
    "title(num)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-WS5WPEOZ9b"
   },
   "outputs": [],
   "source": [
    "figure()\n",
    "a=hist(np.ravel(expcls_test[:,2:]-mycls_test[:,2:]), bins=100, alpha=0.5, label='Anafast')\n",
    "a=hist(np.ravel(result[:,2:]-mycls_test[:,2:]), bins=100, alpha=0.5, label = 'ML')\n",
    "yscale('log')\n",
    "legend()\n",
    "\n",
    "ch2anafast = np.sum((expcls_test[:,2:]-mycls_test[:,2:])**2, axis=1)\n",
    "ch2ML = np.sum((result[:,2:]-mycls_test[:,2:])**2, axis=1)\n",
    "\n",
    "figure()\n",
    "a=hist(ch2anafast, bins=100, alpha=0.5, label='Anafast')\n",
    "a=hist(ch2ML, bins=100, alpha=0.5, label = 'ML')\n",
    "yscale('log')\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNl3317hRi4f"
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "import keras as kr\n",
    "import datetime\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "today = datetime.datetime.now().strftime('%Y%m%d_%H_%M_%S')\n",
    "out_dir=\"/content/gdrive/My Drive/Colab Notebooks/\"\n",
    "\n",
    "kr.models.save_model(model, out_dir +  today + \"_mymodel_CNN.h5py.File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zthM1tP6VMwe"
   },
   "outputs": [],
   "source": [
    "#np.save(out_dir + today + \"_loss-alm\", history.history['loss'])\n",
    "#np.save(out_dir + today + \"_val_loss-alm\", history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl1_uBtCXSWy"
   },
   "outputs": [],
   "source": [
    "#np.save(out_dir + today + \"_mymaps\", mymaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1nca46YYhGI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUMjgmETG9Lv"
   },
   "source": [
    "# Tests sur réseaux denses avec mymaps en input\n",
    "\n",
    "``` python\n",
    "model = Sequential()\n",
    "\n",
    "from keras.layers import Dense\n",
    "model.add(Dense(units=nalm*6,activation='relu',input_dim=npixok,kernel_initializer='uniform'))\n",
    "model.add(Dense(units=nalm*3, activation='relu'))\n",
    "model.add(Dense(units=nl, activation='linear'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "# Test 1:\n",
    "Deux couches dense de nalm*6 et nl neurones avec nl = 2*ns et nalm=(nl)*(nl+1)/2.\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un spli de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats1: \n",
    "Erreur absolue moyenne relative de :\n",
    "### 20,53%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,30%\n",
    "\n",
    "# Test 2:\n",
    "Deux couches dense de nalm10 et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats2:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 45,77%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,30%\n",
    "\n",
    "# Test3:\n",
    "Deux couches dense de nalm3 et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats3:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 61,00%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22,32%\n",
    "\n",
    "En changeant le nombre de neurones de la première couche on a une modification de la performance mais pas de différence notable sur le chi2 ou l'histogramme de l'erreur si ce n'est des valeur légèrement différente (en accord ave la nouvelle erreur). Il semble que 6*nalme soit la meilleur quantité de neurones.\n",
    "\n",
    "\n",
    "\n",
    "# Test4:\n",
    "Trois couches dense de nalm*6, nalm*6 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats4:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 97.36667359705528%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.323505638289156%\n",
    "\n",
    "# Test5:\n",
    "Trois couches dense de nalm*6, nalm*8 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats5:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 70.17615607561682%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.323505638289156%\n",
    "\n",
    "# Test6:\n",
    "Trois couches dense de nalm*6, nalm*12 (act = linear) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats6:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 69.0112935047282%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.337909475916774%\n",
    "\n",
    "On a ajouté une couche de neurone dense. En faisant varier la quantié de neurones on observe\n",
    "\n",
    "# Test7:\n",
    "Trois couches dense de nalm*6, nalm*6 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats7:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 14.372369414690606%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.321932372640372%\n",
    "\n",
    "# Test8:\n",
    "Trois couches dense de nalm*6, nalm*8 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats8:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 15.105077069416465%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.276099533297497%\n",
    "\n",
    "# Test9:\n",
    "Trois couches dense de nalm*6, nalm*4 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats9:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 13.116186779395962%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### 22.294983163667492%\n",
    "\n",
    "# Test10:\n",
    "Trois couches dense de nalm*6, nalm*2 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats10:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 18.824813082983688%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n",
    "\n",
    "# Test11:\n",
    "Trois couches dense de nalm*6, nalm*3 (act = relu) et nl neurones\n",
    "Entrainement avec 80% des données soit 80 000 cartes et un split de 0,1.\n",
    "100 epoch d'entrainement.\n",
    "\n",
    "## Resultats11:\n",
    "Erreur absolue moyenne relative de :\n",
    "### 13.89%. \n",
    "Erreur absolue moyenne relative sur anafast de :\n",
    "### ~22.294983163667492%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqPKY6VRBJHS"
   },
   "source": [
    "# Conclusion\n",
    "Au vu des tests réalisés, on observe qu'en conservant le nombre de neurones originaux pour la premiere couche, le programme est le plus performant (pour cette configuration). On remarque également qu'en ajoutant une couche dense de 4nalm neurones avec une fonction d'activation 'relu', les performance s'acroissent fortement (7% de gain). On pourrait ajouter de nouvelles couches denses 'relu' et faire varier leurs nombres de neurones pour améliorer les performances et diminuer l'erreur, cependant le temps necessaire pour réaliser les opération de la part de GGL Colab étant déjà long pour une couche supplémentaire on peut se douter qu'icelui augmentera encore. Au final si nous mettons plus de couches nous pourrions avoir un résultat plus smooth comme obtenu avec le CNN mais pour un temps beacoup plus long.\n",
    "\n",
    "Les différences notables entre les résultats de cette architecture et celle des réseau convolutionels se trouve sur le fit des Cl (normal). Avec le réseau dense on retrouve grossièrement le spectre recherché mais avec de légères oscillations semblable à du bruit. Avec le CNN on retrouve le spectre attendu mais avec des pics spontanés à certains points du spectre. Le manque de test sur le CNN pour ce problème ne me permet pas de le comparer efficacement avec le réseau dense, mais offre une idée des différences potentielle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfoyXYywBM0c"
   },
   "source": [
    "# Tests avec réseau CNN NNhealpix, my maps en inputs\n",
    "\n",
    "# Test12:\n",
    "Test sur le réseau identique à celui de l'article avec 100k data. Degrade jusqu'à 1.\n",
    "\n",
    "\n",
    "\n",
    "## Résultats12:\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EssaiFullSkMap.ipynb",
   "provenance": [
    {
     "file_id": "1HyvrlFTBjMiNWV_WSdSZ1TS4HGYh099Y",
     "timestamp": 1561015732593
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
